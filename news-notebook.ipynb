{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6a9a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3b5df",
   "metadata": {},
   "source": [
    "## Subcorpus 2: PBS News Transcripts\n",
    "\n",
    "The text is extracted from 1,000 news videos from PBS News Hour and Washington Week, available at https://www.zerotohero.ca/zh/en/show/talk/293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd0bc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the text\n",
    "newsFile = open(\"data/news-1000-trunc.txt\", \"r\")\n",
    "newsText = newsFile.read()\n",
    "newsTokenized = word_tokenize(newsText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23209c",
   "metadata": {},
   "source": [
    "1. The length (in words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ff594d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137356\n"
     ]
    }
   ],
   "source": [
    "newsTokens = len(newsTokenized)\n",
    "print(newsTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe18df3",
   "metadata": {},
   "source": [
    "2. The lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e58dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0805425318151373\n"
     ]
    }
   ],
   "source": [
    "newsTypes = len(set(newsTokenized))\n",
    "newsLexDiversity = newsTypes / newsTokens\n",
    "print(newsLexDiversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c908eb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. The longest sentence (type the sentence and also give the number of words). Hint: look at the Gutenberg part of Section 2.1 in NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "494007bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest 'sentence' is:\n",
      "\n",
      "\n",
      "NICK SCHIFRIN: Last month, the Pentagon made\n",
      "public a 2020 report that admitted: “White supremacy and white nationalism pose a threat\n",
      "to the good order and discipline within the military and individuals with extremist affiliations\n",
      "and military experience are a concern to U.S. national security.” But it also concluded: “We believe we have\n",
      "been effective at screening for individuals who possess or advocate extremist ideologies.” LECIA BROOKS, Chief of Staff, Southern Poverty\n",
      "Law Center: We’re happy to see that the Pentagon agrees that there’s a problem, but we completely\n",
      "disagree that they’re doing anything about it.\n",
      "\n",
      "\n",
      "\n",
      "The sentence has 115 words.\n"
     ]
    }
   ],
   "source": [
    "newsSentences = nltk.sent_tokenize(newsText)\n",
    "word_count = lambda sentence: len(nltk.word_tokenize(sentence))\n",
    "print(\"The longest 'sentence' is:\\n\\n\")\n",
    "newsLongestSentence = max(newsSentences, key=word_count)\n",
    "print(newsLongestSentence)\n",
    "print(\"\\n\\n\")\n",
    "print(f\"The sentence has {word_count(newsLongestSentence)} words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ce2c7",
   "metadata": {},
   "source": [
    "\n",
    "4. The top collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc3461da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUDY WOODRUFF; NICK SCHIFRIN; JOHN YANG; AMNA NAWAZ; United States;\n",
      "YAMICHE ALCINDOR; White House; WILLIAM BRANGHAM; PBS NewsHour;\n",
      "President Biden; PAUL SOLMAN; New York; SAM LAZARO; George Floyd; Hong\n",
      "Kong; Biden administration; JEFFREY BROWN; Derek Chauvin; Supreme\n",
      "Court; Judy Woodruff\n"
     ]
    }
   ],
   "source": [
    "newsNLTKText = nltk.Text(newsTokenized)\n",
    "newsNLTKText.collocations(num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42092de9",
   "metadata": {},
   "source": [
    "5. The top ten words that start with each of the vowels (involves using FreqDist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "625b4961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words with 'a': ['and', 'a', 'And', 'are', 'as', 'at', 'about', 'an', 'all', 'also']\n",
      "\n",
      "Top ten words with 'e': ['even', 'every', 'end', 'early', 'election', 'everything', 'economy', 'especially', 'example', 'enough']\n",
      "\n",
      "Top ten words with 'i': ['in', 'is', 'I', 'it', 'It', 'if', 'In', 'into', 'its', 'important']\n",
      "\n",
      "Top ten words with 'o': ['of', 'on', 'out', 'or', 'one', 'our', 'other', 'over', 'only', 'own']\n",
      "\n",
      "Top ten words with 'u': ['up', 'us', 'U.S.', 'United', 'use', 'under', 'until', 'University', 'understand', 'used']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsFreqDist = nltk.probability.FreqDist(newsNLTKText)\n",
    "\n",
    "def startsWithA(word):\n",
    "    return re.search(f\"^[Aa]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithA = list(filter(startsWithA, newsFreqDist))[:10]\n",
    "\n",
    "def startsWithE(word):\n",
    "    return re.search(f\"^[Ee]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithE = list(filter(startsWithE, newsFreqDist))[:10]\n",
    "\n",
    "def startsWithI(word):\n",
    "    return re.search(f\"^[Ii]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithI = list(filter(startsWithI, newsFreqDist))[:10]\n",
    "\n",
    "def startsWithO(word):\n",
    "    return re.search(f\"^[Oo]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithO = list(filter(startsWithO, newsFreqDist))[:10]\n",
    "\n",
    "def startsWithU(word):\n",
    "    return re.search(f\"^[Uu]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithU = list(filter(startsWithU, newsFreqDist))[:10]\n",
    "\n",
    "print(f\"Top ten words with 'a': {newsTopTenWordsWithA}\\n\")\n",
    "print(f\"Top ten words with 'e': {newsTopTenWordsWithE}\\n\")\n",
    "print(f\"Top ten words with 'i': {newsTopTenWordsWithI}\\n\")\n",
    "print(f\"Top ten words with 'o': {newsTopTenWordsWithO}\\n\")\n",
    "print(f\"Top ten words with 'u': {newsTopTenWordsWithU}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aab02b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "6. A stemmed version of the longest sentence (extracted above in 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2aa4639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed version of the longest sentence (but with a space around each punctuation):\n",
      "\n",
      "nick schifrin : last month , the pentagon made public a 2020 report that admit : “ white supremaci and white nation pose a threat to the good order and disciplin within the militari and individu with extremist affili and militari experi are a concern to u.s. nation security. ” but it also conclud : “ we believ we have been effect at screen for individu who possess or advoc extremist ideologies. ” lecia brook , chief of staff , southern poverti law center : we ’ re happi to see that the pentagon agre that there ’ s a problem , but we complet disagre that they ’ re do anyth about it .\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "newsLongestSentenceList = []\n",
    "\n",
    "for w in nltk.word_tokenize(newsLongestSentence):\n",
    "    newsLongestSentenceList.append(ps.stem(w))\n",
    "\n",
    "newsStemmedSentence = \" \".join(newsLongestSentenceList)\n",
    "    \n",
    "print(\"The stemmed version of the longest sentence (but with a space around each punctuation):\\n\")\n",
    "\n",
    "print(newsStemmedSentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
