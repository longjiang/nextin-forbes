{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72345ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb56157",
   "metadata": {},
   "source": [
    "|   Name (Last, First)   |   Student ID   |   Section contributed                                          |   Section edited                                   |   Other contributions                                                      |\n",
    "|------------------------|----------------|----------------------------------------------------------------|----------------------------------------------------|----------------------------------------------------------------------------|\n",
    "|    Jiang Long          |    200099436   |   Contributed to all sections.    (Typed on Jupiter notebook)  |   All sections                                     |   Submission of the text files and assignment files, Truncated text files  |\n",
    "|    Antanila H.         |    301332035   |   Research of texts, contributed more on 2, 3 sections         |   Overall input in all sections, more on 2 and 3   |   Found different codes to input from the internet and lab assignments     |\n",
    "|    Sava Savkovic       |    301397121   |   Contributed to all sections.                                 |   All sections                                     |   Found different codes to input from the internet and lab assignments     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be929ed",
   "metadata": {},
   "source": [
    "## Subcorpus 1: The complete works of William Shakespeare\n",
    "\n",
    "The text is downloaded in .txt format from archive.org: \n",
    "https://archive.org/details/completeworksofw00shakrich\n",
    "\n",
    "The file is truncated to include only the first 9,848 lines because we have slow laptops. :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5abc45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the text\n",
    "shakespeareFile = open(\"data/literature-shakespeare-trunc.txt\", \"r\")\n",
    "shakespeareText = shakespeareFile.read()\n",
    "shakespeareTokenized = word_tokenize(shakespeareText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23209c",
   "metadata": {},
   "source": [
    "1. The length (in words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff594d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49497\n"
     ]
    }
   ],
   "source": [
    "shakespeareTokens = len(shakespeareTokenized)\n",
    "print(shakespeareTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe18df3",
   "metadata": {},
   "source": [
    "2. The lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e58dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1317655615491848\n"
     ]
    }
   ],
   "source": [
    "shakespeareTypes = len(set(shakespeareTokenized))\n",
    "shakespeareLexDiversity = shakespeareTypes / shakespeareTokens\n",
    "print(shakespeareLexDiversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c908eb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. The longest sentence (type the sentence and also give the number of words). Hint: look at the Gutenberg part of Section 2.1 in NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a216939b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest 'sentence' is:\n",
      "\n",
      "\n",
      "Pro, Ye elves of hills, brooks, standing lakes, \n",
      "and groves; \n",
      "\n",
      "And ye that on, the sands with printless foot \n",
      "Do chase the ebbing Neptune, and do fly him \n",
      "When he comes back; you demi-puppets that \n",
      "By moonshine do the green sour ringlets make, \n",
      "Whereoftkeewenotbites;andyouwhosepastime \n",
      "Is to make midnight mushrooms, that rejoice \n",
      "To hear the solemn curfew; by whose aid, — \n",
      "Weak masters thou^ ye be,— I haVe bedimm’d \n",
      "Thenoontidesun,call’dforththemutinous winds, \n",
      "And ’twixt the green sea and the azured vault \n",
      "Set roaring war: to the dread rattling thunder \n",
      "Have I given fire, and rifted Jove’s stout oak \n",
      "With his own bolt: the strong-based promontory \n",
      "Have I made shake: and by the spurs pluck’d up \n",
      "The pine and cedar: graves, at my command, \n",
      "Have waked their sleepers, oped, and let them \n",
      "forth \n",
      "\n",
      "By my so potent art.\n",
      "\n",
      "\n",
      "\n",
      "The sentence has 173 words.\n"
     ]
    }
   ],
   "source": [
    "shakespeareSentences = nltk.sent_tokenize(shakespeareText)\n",
    "print(\"The longest 'sentence' is:\\n\\n\")\n",
    "\n",
    "# We got this from https://stackoverflow.com/questions/27652187/python-finding-the-longest-shortest-sentence-in-a-random-paragraph\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "shakespeareLongestSentence = max(shakespeareSentences, key=word_count)\n",
    "\n",
    "print(shakespeareLongestSentence)\n",
    "print(\"\\n\\n\")\n",
    "print(f\"The sentence has {word_count(shakespeareLongestSentence)} words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ce2c7",
   "metadata": {},
   "source": [
    "\n",
    "4. The top collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fec9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWO GENTLEMEN; thou art; Sir Proteus; thou hast; Sir Thurio; thou\n",
      "canst; Scene I.—; Sir Valentine; Sir John; Master Page; thou beest;\n",
      "widow Dido; Thou liest; Wilt thou; pray thee; PERSONS REPRESENTED;\n",
      "Pro- teus; John Falstaff; Thou hast; Dost thou\n"
     ]
    }
   ],
   "source": [
    "shakespeareNLTKText = nltk.Text(shakespeareTokenized)\n",
    "shakespeareNLTKText.collocations(num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42092de9",
   "metadata": {},
   "source": [
    "5. The top ten words that start with each of the vowels (involves using FreqDist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "199b2901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words with 'a': ['and', 'a', 'And', 'as', 'are', 'all', 'at', 'am', 'A', 'Ant']\n",
      "\n",
      "Top ten words with 'e': ['Enter', 'else', 'Exit', 'er', 'Exeunt', 'ever', 'eyes', 'earth', 'even', 'end']\n",
      "\n",
      "Top ten words with 'i': ['I', 'is', 'in', 'it', 'if', 'If', 'It', 'Is', 'In', 'indeed']\n",
      "\n",
      "Top ten words with 'o': ['of', 'on', 'one', 'our', 'or', 'out', 'Out', 'Of', 'O', 'own']\n",
      "\n",
      "Top ten words with 'u': ['upon', 'us', 'up', 'use', 'U', 'unto', 'Upon', 'under', 'Unless', 'Under']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shakespeareFreqDist = nltk.probability.FreqDist(shakespeareNLTKText)\n",
    "\n",
    "def startsWithA(word):\n",
    "    return re.search(f\"^[Aa]\", word) != None\n",
    "\n",
    "shakespeareTopTenWordsWithA = list(filter(startsWithA, shakespeareFreqDist))[:10]\n",
    "\n",
    "def startsWithE(word):\n",
    "    return re.search(f\"^[Ee]\", word) != None\n",
    "\n",
    "shakespeareTopTenWordsWithE = list(filter(startsWithE, shakespeareFreqDist))[:10]\n",
    "\n",
    "def startsWithI(word):\n",
    "    return re.search(f\"^[Ii]\", word) != None\n",
    "\n",
    "shakespeareTopTenWordsWithI = list(filter(startsWithI, shakespeareFreqDist))[:10]\n",
    "\n",
    "def startsWithO(word):\n",
    "    return re.search(f\"^[Oo]\", word) != None\n",
    "\n",
    "shakespeareTopTenWordsWithO = list(filter(startsWithO, shakespeareFreqDist))[:10]\n",
    "\n",
    "def startsWithU(word):\n",
    "    return re.search(f\"^[Uu]\", word) != None\n",
    "\n",
    "shakespeareTopTenWordsWithU = list(filter(startsWithU, shakespeareFreqDist))[:10]\n",
    "\n",
    "print(f\"Top ten words with 'a': {shakespeareTopTenWordsWithA}\\n\")\n",
    "print(f\"Top ten words with 'e': {shakespeareTopTenWordsWithE}\\n\")\n",
    "print(f\"Top ten words with 'i': {shakespeareTopTenWordsWithI}\\n\")\n",
    "print(f\"Top ten words with 'o': {shakespeareTopTenWordsWithO}\\n\")\n",
    "print(f\"Top ten words with 'u': {shakespeareTopTenWordsWithU}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aab02b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "6. A stemmed version of the longest sentence (extracted above in 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf13cf86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed version of the longest sentence (but with a space around each punctuation):\n",
      "\n",
      "pro , ye elv of hill , brook , stand lake , and grove ; and ye that on , the sand with printless foot do chase the eb neptun , and do fli him when he come back ; you demi-puppet that by moonshin do the green sour ringlet make , whereoftkeewenotbit ; andyouwhosepastim is to make midnight mushroom , that rejoic to hear the solemn curfew ; by whose aid , — weak master thou^ ye be , — i have bedimm ’ d thenoontidesun , call ’ dforththemutin wind , and ’ twixt the green sea and the azur vault set roar war : to the dread rattl thunder have i given fire , and rift jove ’ s stout oak with hi own bolt : the strong-bas promontori have i made shake : and by the spur pluck ’ d up the pine and cedar : grave , at my command , have wake their sleeper , ope , and let them forth by my so potent art .\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "shakespeareLongestSentenceList = []\n",
    "\n",
    "for w in nltk.word_tokenize(shakespeareLongestSentence):\n",
    "    shakespeareLongestSentenceList.append(ps.stem(w))\n",
    "\n",
    "shakespeareStemmedSentence = \" \".join(shakespeareLongestSentenceList)\n",
    "    \n",
    "print(\"The stemmed version of the longest sentence (but with a space around each punctuation):\\n\")\n",
    "\n",
    "print(shakespeareStemmedSentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3b5df",
   "metadata": {},
   "source": [
    "## Subcorpus 2: PBS News Transcripts\n",
    "\n",
    "The text is extracted from 1,000 news videos from PBS News Hour and Washington Week, available at https://www.zerotohero.ca/zh/en/show/talk/293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0bc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the text\n",
    "newsFile = open(\"data/news-1000-trunc.txt\", \"r\")\n",
    "newsText = newsFile.read()\n",
    "newsTokenized = word_tokenize(newsText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168daa43",
   "metadata": {},
   "source": [
    "1. The length (in words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93fa2dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137356\n"
     ]
    }
   ],
   "source": [
    "newsTokens = len(newsTokenized)\n",
    "print(newsTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91c1c6",
   "metadata": {},
   "source": [
    "2. The lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff1c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0805425318151373\n"
     ]
    }
   ],
   "source": [
    "newsTypes = len(set(newsTokenized))\n",
    "newsLexDiversity = newsTypes / newsTokens\n",
    "print(newsLexDiversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4766e4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. The longest sentence (type the sentence and also give the number of words). Hint: look at the Gutenberg part of Section 2.1 in NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "494007bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest 'sentence' is:\n",
      "\n",
      "\n",
      "NICK SCHIFRIN: Last month, the Pentagon made\n",
      "public a 2020 report that admitted: “White supremacy and white nationalism pose a threat\n",
      "to the good order and discipline within the military and individuals with extremist affiliations\n",
      "and military experience are a concern to U.S. national security.” But it also concluded: “We believe we have\n",
      "been effective at screening for individuals who possess or advocate extremist ideologies.” LECIA BROOKS, Chief of Staff, Southern Poverty\n",
      "Law Center: We’re happy to see that the Pentagon agrees that there’s a problem, but we completely\n",
      "disagree that they’re doing anything about it.\n",
      "\n",
      "\n",
      "\n",
      "The sentence has 115 words.\n"
     ]
    }
   ],
   "source": [
    "newsSentences = nltk.sent_tokenize(newsText)\n",
    "word_count = lambda sentence: len(nltk.word_tokenize(sentence))\n",
    "print(\"The longest 'sentence' is:\\n\\n\")\n",
    "newsLongestSentence = max(newsSentences, key=word_count)\n",
    "print(newsLongestSentence)\n",
    "print(\"\\n\\n\")\n",
    "print(f\"The sentence has {word_count(newsLongestSentence)} words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b7c9e",
   "metadata": {},
   "source": [
    "\n",
    "4. The top collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc3461da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUDY WOODRUFF; NICK SCHIFRIN; JOHN YANG; AMNA NAWAZ; United States;\n",
      "YAMICHE ALCINDOR; White House; WILLIAM BRANGHAM; PBS NewsHour;\n",
      "President Biden; PAUL SOLMAN; New York; SAM LAZARO; George Floyd; Hong\n",
      "Kong; Biden administration; JEFFREY BROWN; Derek Chauvin; Supreme\n",
      "Court; Judy Woodruff\n"
     ]
    }
   ],
   "source": [
    "newsNLTKText = nltk.Text(newsTokenized)\n",
    "newsNLTKText.collocations(num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f559e38",
   "metadata": {},
   "source": [
    "5. The top ten words that start with each of the vowels (involves using FreqDist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "625b4961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words with 'a': ['and', 'a', 'And', 'are', 'as', 'at', 'about', 'an', 'all', 'also']\n",
      "\n",
      "Top ten words with 'e': ['even', 'every', 'end', 'early', 'election', 'everything', 'economy', 'especially', 'example', 'enough']\n",
      "\n",
      "Top ten words with 'i': ['in', 'is', 'I', 'it', 'It', 'if', 'In', 'into', 'its', 'important']\n",
      "\n",
      "Top ten words with 'o': ['of', 'on', 'out', 'or', 'one', 'our', 'other', 'over', 'only', 'own']\n",
      "\n",
      "Top ten words with 'u': ['up', 'us', 'U.S.', 'United', 'use', 'under', 'until', 'University', 'understand', 'used']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsFreqDist = nltk.probability.FreqDist(newsNLTKText)\n",
    "\n",
    "def startsWithA(word):\n",
    "    return re.search(f\"^[Aa]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithA = list(filter(startsWithA, newsFreqDist))[:10]\n",
    "\n",
    "def startsWithE(word):\n",
    "    return re.search(f\"^[Ee]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithE = list(filter(startsWithE, newsFreqDist))[:10]\n",
    "\n",
    "def startsWithI(word):\n",
    "    return re.search(f\"^[Ii]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithI = list(filter(startsWithI, newsFreqDist))[:10]\n",
    "\n",
    "def startsWithO(word):\n",
    "    return re.search(f\"^[Oo]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithO = list(filter(startsWithO, newsFreqDist))[:10]\n",
    "\n",
    "def startsWithU(word):\n",
    "    return re.search(f\"^[Uu]\", word) != None\n",
    "\n",
    "newsTopTenWordsWithU = list(filter(startsWithU, newsFreqDist))[:10]\n",
    "\n",
    "print(f\"Top ten words with 'a': {newsTopTenWordsWithA}\\n\")\n",
    "print(f\"Top ten words with 'e': {newsTopTenWordsWithE}\\n\")\n",
    "print(f\"Top ten words with 'i': {newsTopTenWordsWithI}\\n\")\n",
    "print(f\"Top ten words with 'o': {newsTopTenWordsWithO}\\n\")\n",
    "print(f\"Top ten words with 'u': {newsTopTenWordsWithU}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aab02b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "6. A stemmed version of the longest sentence (extracted above in 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2aa4639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed version of the longest sentence (but with a space around each punctuation):\n",
      "\n",
      "nick schifrin : last month , the pentagon made public a 2020 report that admit : “ white supremaci and white nation pose a threat to the good order and disciplin within the militari and individu with extremist affili and militari experi are a concern to u.s. nation security. ” but it also conclud : “ we believ we have been effect at screen for individu who possess or advoc extremist ideologies. ” lecia brook , chief of staff , southern poverti law center : we ’ re happi to see that the pentagon agre that there ’ s a problem , but we complet disagre that they ’ re do anyth about it .\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "newsLongestSentenceList = []\n",
    "\n",
    "for w in nltk.word_tokenize(newsLongestSentence):\n",
    "    newsLongestSentenceList.append(ps.stem(w))\n",
    "\n",
    "newsStemmedSentence = \" \".join(newsLongestSentenceList)\n",
    "    \n",
    "print(\"The stemmed version of the longest sentence (but with a space around each punctuation):\\n\")\n",
    "\n",
    "print(newsStemmedSentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3b5df",
   "metadata": {},
   "source": [
    "## Subcorpus 3: Straight Outta Compton\n",
    "\n",
    "This is Straight Outta Compton Screenplay by Jonathan Herman and Andrea Berloff, downloaded from https://archive.org/details/StraightOuttaComptonScreenplayByJonathanHermanAndAndreaBerloff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0bc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the text\n",
    "comptonFile = open(\"data/straight-outta-compton.txt\", \"r\")\n",
    "comptonText = comptonFile.read()\n",
    "comptonTokenized = word_tokenize(comptonText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23209c",
   "metadata": {},
   "source": [
    "1. The length (in words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ff594d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40887\n"
     ]
    }
   ],
   "source": [
    "comptonTokens = len(comptonTokenized)\n",
    "print(comptonTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe18df3",
   "metadata": {},
   "source": [
    "2. The lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e58dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1398243940616822\n"
     ]
    }
   ],
   "source": [
    "comptonTypes = len(set(comptonTokenized))\n",
    "comptonLexDiversity = comptonTypes / comptonTokens\n",
    "print(comptonLexDiversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c908eb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. The longest sentence (type the sentence and also give the number of words). Hint: look at the Gutenberg part of Section 2.1 in NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "494007bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest 'sentence' is:\n",
      "\n",
      "\n",
      "PROD #02443 \n",
      "\n",
      "\n",
      "Director: F. Gary Gray \n",
      "Producer: Ice Cube \n",
      "Producer: Tomica Woods-Wright \n",
      "Producer: Matt Alvarez \n",
      "Producer: F. Gary Gray \n",
      "Producer: Scott Bernstein \n",
      "Producer: Dr. Dre \n",
      "Executive Producer: Adam Merims \n",
      "\n",
      "\n",
      "STRAIGHT OUTTA COMPTON \n",
      "\n",
      "\n",
      "Screenplay by \n",
      "\n",
      "Jonathan Herman and Andrea Berloff \n",
      "Story by \n",
      "\n",
      "S. Leigh Savidge & Alan Wenkus and Andrea Berloff \n",
      "\n",
      "\n",
      "Notice : \n",
      "\n",
      "This material is the property of Straight Outta LLC (A wholly \n",
      "owned subsidiary of Universal City Studios, Inc.) and is intended \n",
      "and restricted solely for studio use by studio personnel.\n",
      "\n",
      "\n",
      "\n",
      "The sentence has 95 words.\n"
     ]
    }
   ],
   "source": [
    "comptonSentences = nltk.sent_tokenize(comptonText)\n",
    "word_count = lambda sentence: len(nltk.word_tokenize(sentence))\n",
    "print(\"The longest 'sentence' is:\\n\\n\")\n",
    "comptonLongestSentence = max(comptonSentences, key=word_count)\n",
    "print(comptonLongestSentence)\n",
    "print(\"\\n\\n\")\n",
    "print(f\"The sentence has {word_count(comptonLongestSentence)} words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ce2c7",
   "metadata": {},
   "source": [
    "\n",
    "4. The top collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3461da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRYAN TURNER; LOS ANGELES; Death Row; AUDIO ACHIEVEMENTS; TORRANCE\n",
      "COP; STRAIGHT OUTTA; OUTTA COMPTON; TOUR BUS; ACHIEVEMENTS STUDIO;\n",
      "MOMENTS LATER; LENCH MOB; Jerry Heller; JIMMY IOVINE; n't even; JERRY\n",
      "HELLER; n't believe; n't know; DEATH ROW; Jheri curl; NEW YORK\n"
     ]
    }
   ],
   "source": [
    "comptonNLTKText = nltk.Text(comptonTokenized)\n",
    "comptonNLTKText.collocations(num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42092de9",
   "metadata": {},
   "source": [
    "5. The top ten words that start with each of the vowels (involves using FreqDist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "625b4961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words with 'a': ['a', 'and', 'at', 'as', 'all', 'about', 'A', 'are', 'And', 'around']\n",
      "\n",
      "Top ten words with 'e': ['Eazy', 'EAZY', 'eyes', 'EXT', 'Eric', 'even', 'each', 'exits', 'ever', 'everything']\n",
      "\n",
      "Top ten words with 'i': ['I', 'in', 'it', 'is', 'INT', 'It', 'into', 'if', 'IN', 'INTO']\n",
      "\n",
      "Top ten words with 'o': ['of', 'on', 'out', 'over', 'off', 'one', 'other', 'ON', 'or', 'OF']\n",
      "\n",
      "Top ten words with 'u': ['up', 'us', 'UP', 'uckin', 'under', 'until', 'Until', 'upon', 'UNIFORM', 'ucka']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comptonFreqDist = nltk.probability.FreqDist(comptonNLTKText)\n",
    "\n",
    "def startsWithA(word):\n",
    "    return re.search(f\"^[Aa]\", word) != None\n",
    "\n",
    "comptonTopTenWordsWithA = list(filter(startsWithA, comptonFreqDist))[:10]\n",
    "\n",
    "def startsWithE(word):\n",
    "    return re.search(f\"^[Ee]\", word) != None\n",
    "\n",
    "comptonTopTenWordsWithE = list(filter(startsWithE, comptonFreqDist))[:10]\n",
    "\n",
    "def startsWithI(word):\n",
    "    return re.search(f\"^[Ii]\", word) != None\n",
    "\n",
    "comptonTopTenWordsWithI = list(filter(startsWithI, comptonFreqDist))[:10]\n",
    "\n",
    "def startsWithO(word):\n",
    "    return re.search(f\"^[Oo]\", word) != None\n",
    "\n",
    "comptonTopTenWordsWithO = list(filter(startsWithO, comptonFreqDist))[:10]\n",
    "\n",
    "def startsWithU(word):\n",
    "    return re.search(f\"^[Uu]\", word) != None\n",
    "\n",
    "comptonTopTenWordsWithU = list(filter(startsWithU, comptonFreqDist))[:10]\n",
    "\n",
    "print(f\"Top ten words with 'a': {comptonTopTenWordsWithA}\\n\")\n",
    "print(f\"Top ten words with 'e': {comptonTopTenWordsWithE}\\n\")\n",
    "print(f\"Top ten words with 'i': {comptonTopTenWordsWithI}\\n\")\n",
    "print(f\"Top ten words with 'o': {comptonTopTenWordsWithO}\\n\")\n",
    "print(f\"Top ten words with 'u': {comptonTopTenWordsWithU}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aab02b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "6. A stemmed version of the longest sentence (extracted above in 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2aa4639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed version of the longest sentence (but with a space around each punctuation):\n",
      "\n",
      "prod # 02443 director : f. gari gray produc : ice cube produc : tomica woods-wright produc : matt alvarez produc : f. gari gray produc : scott bernstein produc : dr. dre execut produc : adam merim straight outta compton screenplay by jonathan herman and andrea berloff stori by s. leigh savidg & alan wenku and andrea berloff notic : thi materi is the properti of straight outta llc ( a wholli own subsidiari of univers citi studio , inc. ) and is intend and restrict sole for studio use by studio personnel .\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "comptonLongestSentenceList = []\n",
    "\n",
    "for w in nltk.word_tokenize(comptonLongestSentence):\n",
    "    comptonLongestSentenceList.append(ps.stem(w))\n",
    "\n",
    "comptonStemmedSentence = \" \".join(comptonLongestSentenceList)\n",
    "    \n",
    "print(\"The stemmed version of the longest sentence (but with a space around each punctuation):\\n\")\n",
    "\n",
    "print(comptonStemmedSentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
